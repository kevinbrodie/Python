{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Bring in all of our dependencies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-02 17:07:41,621] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# We should define the environment here and detremine the sizing so we can generalize our simple neural network\n",
    "env    = gym.make('CartPole-v0')   # Choose our environment\n",
    "tmp    = env.action_space.sample()\n",
    "print(type(tmp))\n",
    "action = np.array([tmp])\n",
    "N      = len(action)               # Use this to generalize the input size for the NN\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a very simple neural network, reLU layers have been shown to train faster. We only have one hidden layer here, very simple\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=((4,)), activation='relu',init='uniform'))\n",
    "model.add(Dense(100, activation='relu',init='uniform'))\n",
    "model.add(Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model for training, we can tweak our loss function and optimizer for different environments\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 13 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 56 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 77 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 48 timesteps\n",
      "Episode finished after 64 timesteps\n",
      "Episode finished after 369 timesteps\n",
      "Episode finished after 345 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 351 timesteps\n",
      "Episode finished after 341 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 440 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 422 timesteps\n",
      "Episode finished after 431 timesteps\n",
      "Episode finished after 438 timesteps\n",
      "Episode finished after 442 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Initialize replay memory\n",
    "#repMem = np.empty((0,4))\n",
    "runAvg = np.zeros((5,1))\n",
    "gamma = 0.7\n",
    "epsUpdate = 0.05\n",
    "maxMem = 1000\n",
    "obs1 = env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs2, reward, done, info = env.step(action)\n",
    "repMem = np.array([obs1,action,reward,obs2,done])\n",
    "tFlag = 0\n",
    "fSkip = 0\n",
    "\n",
    "#Start monigtoring\n",
    "#env.monitor.start('/tmp/cartpole-experiment-1')\n",
    "\n",
    "#repMem = [1,2,3,4]\n",
    "# Run episodes tally results\n",
    "for i_episode in range(30):\n",
    "    obs1 = env.reset()\n",
    "    epsilon = epsUpdate * i_episode\n",
    "    rTally = np.array([0])\n",
    "    loss = 0.\n",
    "    \n",
    "    for t in range(451):\n",
    "        env.render()\n",
    "        obs2 = obs1\n",
    "        \n",
    "        # Add action selection by max output of network over time, update epsilon slowly\n",
    "        if np.random.uniform(0,1,1) >= min(0.9,epsilon):\n",
    "            action = env.action_space.sample()                    # this is a random action\n",
    "        else:\n",
    "            action = int(np.argmax(model.predict(obs2[np.newaxis,])[0]))  \n",
    "            \n",
    "        obs1, reward, done, info = env.step(action)    # apply the action to the model and check the outputs\n",
    "        \n",
    "        # Quit early if too many steps\n",
    "        if t == 450:\n",
    "            done = True\n",
    "        \n",
    "        # Training Stuff ===========================================================================\n",
    "        if tFlag == 0:\n",
    "            repMem = np.column_stack((repMem, np.array([obs2,action,reward,obs1,done])))\n",
    "\n",
    "            if repMem.shape[1] > maxMem:\n",
    "                #np.delete(repMem,repMem.shape[1]-1,1)\n",
    "                np.delete(repMem,0,1)\n",
    "\n",
    "            if repMem.shape[1] > 10:\n",
    "                P = max(int(1),math.floor(repMem.shape[1]/5))\n",
    "            else:\n",
    "                P = int(1)\n",
    "\n",
    "            # Get random batch\n",
    "            memBatch = repMem[:,np.random.choice(repMem.shape[1], int(P), replace=False)]\n",
    "\n",
    "\n",
    "            # Put Q-Learning stuff here to start...\n",
    "            # Q[s,a] = Q[s,a] + a(r + y max_a'(Q[s',a'] - Q[s,a]))\n",
    "            # Get targets\n",
    "            tgt = np.zeros((P,2))\n",
    "\n",
    "            for batchNum in range(P):\n",
    "\n",
    "                tmp0 = memBatch[0,batchNum]\n",
    "                tmp0 = tmp0[np.newaxis,]\n",
    "                tgt[batchNum,:] = model.predict(tmp0)[0]\n",
    "\n",
    "                tmp = memBatch[3,batchNum]\n",
    "                tmp = tmp[np.newaxis,]\n",
    "                QVal = np.max(model.predict(tmp)[0])\n",
    "\n",
    "                tmpReward = memBatch[2,batchNum]\n",
    "                tmpA      = memBatch[1,batchNum]\n",
    "\n",
    "                tmpDone = memBatch[4,batchNum]\n",
    "                if tmpDone:\n",
    "                    tgt[batchNum,tmpA] = tmpReward\n",
    "                else:\n",
    "                    tgt[batchNum,tmpA] = tmpReward + (gamma * QVal)     \n",
    "                \n",
    "            tFlag = 1\n",
    "                \n",
    "            # Train              \n",
    "            if memBatch[0,:].shape[0] == 1:\n",
    "                tmp = memBatch[0,]\n",
    "                tmp = tmp[0]\n",
    "                tmp = tmp[np.newaxis,]\n",
    "            else:\n",
    "                tmp = np.vstack(memBatch[0,:])\n",
    "\n",
    "            loss += model.train_on_batch(tmp, tgt)\n",
    "            \n",
    "        else:\n",
    "            tFlag = 0\n",
    "          \n",
    "        # ========================================================================\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "\n",
    "            if i_episode <= 3:\n",
    "                runAvg[i_episode] = t+1\n",
    "            else:\n",
    "                runAvg[0:3] = runAvg[1:4]\n",
    "                runAvg[4] = t+1\n",
    "                #print(\"Running Average: {} \".format(np.sum(runAvg)/5)) \n",
    "                #if np.sum(runAvg)/5 >=195:\n",
    "                    #tFlag = 1\n",
    "        \n",
    "            break\n",
    "            \n",
    "model.save('my_model.h5')            \n",
    "#env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
