{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bring in all of our dependencies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-26 17:05:46,645] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# We should define the environment here and detremine the sizing so we can generalize our simple neural network\n",
    "env    = gym.make('CartPole-v0')   # Choose our environment\n",
    "tmp    = env.action_space.sample()\n",
    "print(type(tmp))\n",
    "action = np.array([tmp])\n",
    "N      = len(action)               # Use this to generalize the input size for the NN\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a very simple neural network, reLU layers have been shown to train faster. We only have one hidden layer here, very simple\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=((4,)), activation='relu',init='uniform'))\n",
    "model.add(Dense(100, activation='relu',init='uniform'))\n",
    "model.add(Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model for training, we can tweak our loss function and optimizer for different environments\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-26 17:13:21,327] Creating monitor directory /tmp/cartpole-experiment-1\n",
      "[2016-11-26 17:13:21,339] Starting new video recorder writing to C:\\tmp\\cartpole-experiment-1\\openaigym.video.6.5616.video000000.mp4\n",
      "[2016-11-26 17:13:21,472] Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\tmp\\\\cartpole-experiment-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 26 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 49 timesteps\n",
      "Episode finished after 66 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 55 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 34 timesteps\n",
      "Episode finished after 107 timesteps\n",
      "Episode finished after 145 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Initialize replay memory\n",
    "#repMem = np.empty((0,4))\n",
    "gamma = 0.8\n",
    "epsUpdate = 0.1\n",
    "maxMem = 200\n",
    "obs1 = env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs2, reward, done, info = env.step(action)\n",
    "repMem = np.array([obs1,action,reward,obs2,done])\n",
    "\n",
    "#Start monigtoring\n",
    "env.monitor.start('/tmp/cartpole-experiment-1')\n",
    "\n",
    "#repMem = [1,2,3,4]\n",
    "# Run episodes tally results\n",
    "for i_episode in range(500):\n",
    "    obs1 = env.reset()\n",
    "    epsilon = epsUpdate * i_episode\n",
    "    rTally = np.array([0])\n",
    "    loss = 0.\n",
    "    \n",
    "    for t in range(300):\n",
    "        env.render()\n",
    "        obs2 = obs1\n",
    "        \n",
    "        # Add action selection by max output of network over time, update epsilon slowly\n",
    "        if np.random.uniform(0,1,1) >= (epsilon):\n",
    "            action = env.action_space.sample()                    # this is a random action\n",
    "        else:\n",
    "            action = int(np.argmax(model.predict(obs2[np.newaxis,])[0]))  \n",
    "            \n",
    "        obs1, reward, done, info = env.step(action)    # apply the action to the model and check the outputs\n",
    "        \n",
    "        # Quit early if too many steps\n",
    "        if t == 250:\n",
    "            done = True\n",
    "            \n",
    "        repMem = np.column_stack((repMem, np.array([obs2,action,reward,obs1,done])))\n",
    "        \n",
    "        if repMem.shape[1] > maxMem:\n",
    "            np.delete(repMem,repMem.shape[1]-1,1)\n",
    "            \n",
    "        if repMem.shape[1] > 10:\n",
    "            P = max(float(1),math.floor(repMem.shape[1]/5))\n",
    "        else:\n",
    "            P = int(1)\n",
    "\n",
    "        # Get random batch\n",
    "        memBatch = repMem[:,np.random.choice(repMem.shape[1], int(P), replace=False)]\n",
    "        \n",
    "            \n",
    "        # Put Q-Learning stuff here to start...\n",
    "        # Q[s,a] = Q[s,a] + a(r + y max_a'(Q[s',a'] - Q[s,a]))\n",
    "        # Get targets\n",
    "        tgt = np.zeros((P,2))\n",
    "        \n",
    "        for batchNum in range(P):\n",
    "            \n",
    "            tmp0 = memBatch[0,batchNum]\n",
    "            tmp0 = tmp0[np.newaxis,]\n",
    "            tgt[batchNum,:] = model.predict(tmp0)[0]\n",
    "            \n",
    "            tmp = memBatch[3,batchNum]\n",
    "            tmp = tmp[np.newaxis,]\n",
    "            QVal = np.max(model.predict(tmp)[0])\n",
    "            \n",
    "            tmpReward = memBatch[2,batchNum]\n",
    "            tmpA      = memBatch[1,batchNum]\n",
    "            \n",
    "            tmpDone = memBatch[4,batchNum]\n",
    "            if tmpDone:\n",
    "                tgt[batchNum,tmpA] = tmpReward\n",
    "            else:\n",
    "                tgt[batchNum,tmpA] = tmpReward + (gamma * QVal)                            \n",
    "        # End batch for\n",
    "        \n",
    "       \n",
    "        # Train              \n",
    "        if memBatch[0,:].shape[0] == 1:\n",
    "            tmp = memBatch[0,]\n",
    "            tmp = tmp[0]\n",
    "            tmp = tmp[np.newaxis,]\n",
    "        else:\n",
    "            tmp = np.vstack(memBatch[0,:])\n",
    "        \n",
    "        loss += model.train_on_batch(tmp, tgt)\n",
    "        #print(loss)\n",
    "            \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "            \n",
    "        env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
